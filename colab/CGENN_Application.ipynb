{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Genaral Imports\n",
        "\n",
        "import os\n",
        "\n",
        "import math\n",
        "import operator\n",
        "import functools\n",
        "import itertools\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "hoAfWUZnrB8p"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Al4Uw1kidGd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce4a9420-646b-4f05-fbf4-5a5e0c5bae0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'clifford-group-equivariant-neural-networks'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 112 (delta 32), reused 23 (delta 23), pack-reused 67\u001b[K\n",
            "Receiving objects: 100% (112/112), 349.95 KiB | 2.99 MiB/s, done.\n",
            "Resolving deltas: 100% (38/38), done.\n"
          ]
        }
      ],
      "source": [
        "#@title Set up git custom lib from git repo\n",
        "\n",
        "os.chdir('/')\n",
        "if not os.path.exists(\"/clifford-group-equivariant-neural-networks\"):\n",
        "    !git clone https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks.git\n",
        "os.chdir(\"/clifford-group-equivariant-neural-networks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6GeT95UFXJt"
      },
      "source": [
        "# Clifford Group Equivariant Neural Networks\n",
        "![image](https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks/raw/master/assets/figure.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8VgLMkydMDT"
      },
      "source": [
        "# Links\n",
        "üìú [ArXiV](https://arxiv.org/abs/2305.11141)\n",
        "\n",
        "üñ•Ô∏è [Github](https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks)\n",
        "\n",
        "ü§ì [Blog Posts](https://davidruhe.github.io/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils\n",
        "\n",
        "In this sectioan are implemented all the utility functioion used to build the fundamntals blocks of the Clifford Geometry NN modules."
      ],
      "metadata": {
        "id": "LAA5A0mHql5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unsqueeze_like(tensor: torch.Tensor, like: torch.Tensor, dim=0):\n",
        "    \"\"\"\n",
        "    Unsqueeze last dimensions of tensor to match another tensor's number of dimensions.\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor): tensor to unsqueeze\n",
        "        like (torch.Tensor): tensor whose dimensions to match\n",
        "        dim: int: starting dim, default: 0.\n",
        "    \"\"\"\n",
        "    n_unsqueezes = like.ndim - tensor.ndim\n",
        "    if n_unsqueezes < 0:\n",
        "        raise ValueError(f\"tensor.ndim={tensor.ndim} > like.ndim={like.ndim}\")\n",
        "    elif n_unsqueezes == 0:\n",
        "        return tensor\n",
        "    else:\n",
        "        return tensor[dim * (slice(None),) + (None,) * n_unsqueezes]\n",
        "\n",
        "\n",
        "# copied from the itertools docs\n",
        "def _powerset(iterable):\n",
        "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
        "    s = list(iterable)\n",
        "    return itertools.chain.from_iterable(\n",
        "        itertools.combinations(s, r) for r in range(len(s) + 1)\n",
        "    )\n",
        "\n",
        "\n",
        "class ShortLexBasisBladeOrder:\n",
        "    def __init__(self, n_vectors):\n",
        "        self.index_to_bitmap = torch.empty(2**n_vectors, dtype=int)\n",
        "        self.grades = torch.empty(2**n_vectors, dtype=int)\n",
        "        self.bitmap_to_index = torch.empty(2**n_vectors, dtype=int)\n",
        "\n",
        "        for i, t in enumerate(_powerset([1 << i for i in range(n_vectors)])):\n",
        "            bitmap = functools.reduce(operator.or_, t, 0)\n",
        "            self.index_to_bitmap[i] = bitmap\n",
        "            self.grades[i] = len(t)\n",
        "            self.bitmap_to_index[bitmap] = i\n",
        "            del t  # enables an optimization inside itertools.combinations\n",
        "\n",
        "\n",
        "def set_bit_indices(x: int):\n",
        "    \"\"\"Iterate over the indices of bits set to 1 in `x`, in ascending order\"\"\"\n",
        "    n = 0\n",
        "    while x > 0:\n",
        "        if x & 1:\n",
        "            yield n\n",
        "        x = x >> 1\n",
        "        n = n + 1\n",
        "\n",
        "\n",
        "def count_set_bits(bitmap: int) -> int:\n",
        "    \"\"\"Counts the number of bits set to 1 in bitmap\"\"\"\n",
        "    count = 0\n",
        "    for i in set_bit_indices(bitmap):\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "\n",
        "def canonical_reordering_sign_euclidean(bitmap_a, bitmap_b):\n",
        "    \"\"\"\n",
        "    Computes the sign for the product of bitmap_a and bitmap_b\n",
        "    assuming a euclidean metric\n",
        "    \"\"\"\n",
        "    a = bitmap_a >> 1\n",
        "    sum_value = 0\n",
        "    while a != 0:\n",
        "        sum_value = sum_value + count_set_bits(a & bitmap_b)\n",
        "        a = a >> 1\n",
        "    if (sum_value & 1) == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "\n",
        "def canonical_reordering_sign(bitmap_a, bitmap_b, metric):\n",
        "    \"\"\"\n",
        "    Computes the sign for the product of bitmap_a and bitmap_b\n",
        "    given the supplied metric\n",
        "    \"\"\"\n",
        "    bitmap = bitmap_a & bitmap_b\n",
        "    output_sign = canonical_reordering_sign_euclidean(bitmap_a, bitmap_b)\n",
        "    i = 0\n",
        "    while bitmap != 0:\n",
        "        if (bitmap & 1) != 0:\n",
        "            output_sign *= metric[i]\n",
        "        i = i + 1\n",
        "        bitmap = bitmap >> 1\n",
        "    return output_sign\n",
        "\n",
        "\n",
        "def gmt_element(bitmap_a, bitmap_b, sig_array):\n",
        "    \"\"\"\n",
        "    Element of the geometric multiplication table given blades a, b.\n",
        "    The implementation used here is described in :cite:`ga4cs` chapter 19.\n",
        "    \"\"\"\n",
        "    output_sign = canonical_reordering_sign(bitmap_a, bitmap_b, sig_array)\n",
        "    output_bitmap = bitmap_a ^ bitmap_b\n",
        "    return output_bitmap, output_sign\n",
        "\n",
        "\n",
        "def construct_gmt(index_to_bitmap, bitmap_to_index, signature):\n",
        "    n = len(index_to_bitmap)\n",
        "    array_length = int(n * n)\n",
        "    coords = torch.zeros((3, array_length), dtype=torch.int)\n",
        "    k_list = coords[0, :]\n",
        "    l_list = coords[1, :]\n",
        "    m_list = coords[2, :]\n",
        "\n",
        "    # use as small a type as possible to minimize type promotion\n",
        "    mult_table_vals = torch.zeros(array_length)\n",
        "\n",
        "    for i in range(n):\n",
        "        bitmap_i = index_to_bitmap[i]\n",
        "\n",
        "        for j in range(n):\n",
        "            bitmap_j = index_to_bitmap[j]\n",
        "            bitmap_v, mul = gmt_element(bitmap_i, bitmap_j, signature)\n",
        "            v = bitmap_to_index[bitmap_v]\n",
        "\n",
        "            list_ind = i * n + j\n",
        "            k_list[list_ind] = i\n",
        "            l_list[list_ind] = v\n",
        "            m_list[list_ind] = j\n",
        "\n",
        "            mult_table_vals[list_ind] = mul\n",
        "\n",
        "    return torch.sparse_coo_tensor(\n",
        "        indices=coords, values=mult_table_vals, size=(n, n, n)\n",
        "    )"
      ],
      "metadata": {
        "id": "Gs5-GWS3q02W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zBBm7VLhurK"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "## THIS PART MUST BE CHANGED\n",
        "Let's first create a dataset. Before we can do so, we have to specify a dimensionality $d$.\n",
        "\n",
        "**Note:** To make the notebook run a bit smoother, we implicitly define the dimension through the length of metric $M$, which we will discuss in a bit.\n",
        "\n",
        "Feel free to adjust this parameter to your needs. Just be cautious: the dimensionality of the Clifford algebra (which we will define later) grows exponentially as $2^d$, so large $d$ will get computationally infeasible.\n",
        "\n",
        "Next, we sample $N$ random vectors $u$ and $v$, and the categorical variables $y$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "djGNemajmc4Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_jeYPtWSf3ol"
      },
      "outputs": [],
      "source": [
        "metric = [1, 1]\n",
        "d = len(metric)\n",
        "N = 1024\n",
        "x = torch.randn(N, 2, d)\n",
        "y = torch.randint(1, 3, (N,))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[42]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOHoa7SUCEWW",
        "outputId": "eb5594da-e61d-4d98-b14d-f9e68347ffd1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.8873, -1.1112],\n",
              "        [ 0.8208, -0.8280]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4_Fj0az6TtJ",
        "outputId": "a8cdd8b0-dc40-45e1-f76b-29eed463f0cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.8873, 0.8208])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x[42][:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-x9s1NK5PSd",
        "outputId": "4b21915c-fd9a-4c83-b809-5893d7565202"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 1, 1,  ..., 2, 2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1rrEyKrfjnQ"
      },
      "source": [
        "## The Metric\n",
        "\n",
        "Now, to compute the target values using our function $f(u, v, y)$, we have to specify how we compute the vector norm and inner product.\n",
        "\n",
        "A well-defined inner product uses a *metric*, which defines *how distances are calculated* in a certain setting.\n",
        "Typically, that is in the *Euclidean* setting, this metric is positive definite, e.g., $M:=\\mathrm{diag\\,}[1, 1]$ in the two-dimensional case.\n",
        "\n",
        "A quadratic form here using matrix notation would look like\n",
        "\n",
        "$$v \\mapsto v ^\\top M v,$$\n",
        "\n",
        "or an inner product\n",
        "\n",
        "$$(u, v) \\mapsto u ^\\top M v.$$\n",
        "\n",
        "This means that it acts as the identity. Hence, it is usually not really taken into account by explicitly writing it down.\n",
        "\n",
        "However, in more exotic settings such as the *Minkowski space* used in special relativity, we can have $M:=\\mathrm{diag\\,} [-1, 1, 1, 1]$. Computing inner products in such a space requires carefully taking into account this metric.\n",
        "\n",
        "In the following, feel free to adjust the metric to your needs, its length should, however, match $d$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wCDcMUSTiJFx"
      },
      "outputs": [],
      "source": [
        "assert len(metric) == d, f\"The dimensionality d should match the metric length.\"\n",
        "\n",
        "f = torch.zeros(N)\n",
        "metric_t = torch.tensor(metric, dtype=torch.float)\n",
        "f[y == 1] = torch.cos(torch.einsum('bi, i, bi->b', x[y==1][:, 0], metric_t, x[y==1][:, 0]).abs().sqrt())\n",
        "f[y == 2] = 1/10 * torch.einsum('bi, i, bi->b', x[y==2][:, 0], metric_t, x[y==2][:, 1]) ** 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzMzzpNWkpWw"
      },
      "source": [
        "Then we compute the function $f(u, v, y)$ for all our datapoints. Computing the vector norm and inner products, we take into account the metric, which can easily be done using `torch.einsum`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f[y == 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXSN6ieXCrvC",
        "outputId": "3e21b363-eb3a-4793-bb2b-99e7e5636ffa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 4.5600e-01,  3.5077e-01,  7.6015e-01,  9.9527e-01,  7.8616e-01,\n",
              "        -8.4514e-02,  9.3260e-01, -2.9277e-01, -8.1253e-01,  1.4767e-01,\n",
              "         5.8321e-01,  4.2188e-01,  7.1053e-01,  1.0768e-01,  1.8702e-02,\n",
              "         8.9654e-01, -5.6695e-01,  5.5888e-01,  4.4886e-01,  9.6224e-01,\n",
              "         9.2613e-01,  6.9317e-01,  1.5226e-01,  5.3694e-01,  3.9556e-01,\n",
              "        -9.3179e-01,  9.2151e-01,  4.8318e-01,  7.4320e-01,  3.7025e-01,\n",
              "        -5.5383e-01,  6.9792e-01, -5.3469e-01, -3.7326e-01,  1.6219e-01,\n",
              "         5.4283e-01,  4.4422e-01,  7.1341e-01, -5.5932e-01,  7.9261e-01,\n",
              "         4.5433e-01,  5.8228e-01,  8.4501e-01,  5.2722e-01, -6.0635e-02,\n",
              "        -3.1374e-01,  3.2908e-01, -9.9911e-01, -9.7567e-02, -1.0300e-02,\n",
              "         3.8935e-01,  6.8634e-01,  6.4484e-01,  3.1521e-01, -2.3551e-02,\n",
              "         5.4783e-01,  7.1550e-01,  2.2239e-01,  6.6890e-01,  8.9749e-01,\n",
              "        -5.1104e-01,  5.7127e-02,  5.5373e-01, -9.9739e-01, -5.4467e-02,\n",
              "         9.1742e-01,  6.2396e-01, -1.0957e-01,  4.4160e-01, -9.8890e-01,\n",
              "        -6.4799e-02, -5.1388e-01,  7.7954e-01,  3.9858e-01,  4.7787e-01,\n",
              "         3.6193e-01,  9.9045e-01, -9.9970e-01,  9.7442e-01,  4.8025e-01,\n",
              "         7.3853e-01,  5.5586e-01,  6.8287e-01,  2.2522e-01, -3.5175e-01,\n",
              "         1.7188e-01,  2.5531e-01,  8.5511e-01,  8.9057e-01, -8.9383e-01,\n",
              "         9.1022e-01,  9.0151e-02, -8.1549e-01,  7.6470e-01, -9.1053e-01,\n",
              "         7.5330e-01,  8.7271e-01,  7.2687e-01,  9.9009e-01,  9.0532e-01,\n",
              "         8.4592e-01,  9.0437e-01,  9.3758e-01,  3.3752e-01,  4.5051e-01,\n",
              "        -1.0723e-01,  6.7654e-01,  9.4605e-01,  3.3417e-01, -5.2949e-01,\n",
              "         5.1081e-01,  2.4792e-01, -8.4267e-02, -4.9463e-01, -9.1719e-01,\n",
              "         2.1308e-01, -9.3634e-01,  4.2185e-01,  4.6402e-02,  3.1717e-02,\n",
              "         9.2873e-01, -8.0405e-01,  7.5978e-01,  3.7780e-01,  8.7030e-01,\n",
              "        -8.3167e-01,  2.2442e-01, -9.9087e-01, -5.6289e-01, -7.6888e-02,\n",
              "         6.7258e-01,  7.2492e-01, -2.7108e-01, -2.6596e-01, -2.3081e-01,\n",
              "         8.9986e-01,  3.2132e-01,  6.8789e-01, -2.3778e-01,  2.8835e-01,\n",
              "         8.9074e-01, -2.0414e-01,  8.2796e-01,  7.1265e-01,  3.0162e-01,\n",
              "         1.0817e-01, -5.6627e-03,  5.7157e-01,  9.8542e-01, -7.0316e-01,\n",
              "         1.0373e-01,  9.6613e-01, -1.7099e-01, -2.7382e-01,  8.2482e-01,\n",
              "         5.6083e-02, -5.3773e-02, -7.3514e-01,  6.0601e-01,  6.7374e-01,\n",
              "         2.0964e-02,  5.1185e-01, -7.6683e-01, -6.7132e-02,  8.3916e-01,\n",
              "         4.5525e-01,  7.7072e-01,  6.9822e-01,  4.3724e-01,  7.6802e-01,\n",
              "         6.0417e-01, -6.0312e-01, -9.8166e-01,  9.7104e-01,  7.1988e-01,\n",
              "         4.2749e-01,  8.0927e-01,  5.1451e-01, -3.5685e-01, -2.4474e-01,\n",
              "         9.7260e-01, -7.3663e-02,  4.5707e-01,  9.3743e-01, -1.9416e-01,\n",
              "         5.3651e-01, -7.0808e-01,  5.1243e-01,  7.8167e-01, -1.5002e-01,\n",
              "         3.4968e-01,  6.6335e-01,  5.9351e-01, -7.8012e-02, -2.2879e-01,\n",
              "         9.1485e-01,  8.1923e-01,  8.6778e-01, -3.4567e-02, -9.1158e-01,\n",
              "         3.8221e-01,  1.0748e-01,  8.3646e-01, -4.2972e-01,  7.5309e-01,\n",
              "         7.8361e-01,  4.0561e-01, -8.0204e-01,  5.5823e-01,  9.1356e-01,\n",
              "         3.1689e-01,  6.9518e-01,  5.1363e-01, -7.0918e-01,  4.0676e-01,\n",
              "         8.5036e-01,  6.9956e-01,  2.7217e-01,  2.6339e-01, -6.9741e-01,\n",
              "        -9.3290e-01,  9.7875e-01,  1.3450e-01, -1.2096e-01,  6.4247e-01,\n",
              "         2.3841e-01,  7.8545e-01, -5.1118e-01, -4.0700e-01,  5.4257e-01,\n",
              "         4.5458e-02,  8.5917e-01, -1.5446e-01,  4.5906e-01,  1.4239e-01,\n",
              "         6.2677e-01, -4.3375e-01, -3.3478e-01, -4.4823e-01, -3.8747e-01,\n",
              "        -1.0019e-01,  7.4276e-01,  7.7792e-01,  7.8923e-01, -1.7709e-01,\n",
              "        -7.3775e-01, -9.2245e-01,  8.9063e-01,  8.9589e-01, -3.2641e-01,\n",
              "        -8.9086e-02,  2.7378e-01,  6.6870e-01,  9.9106e-01,  4.9406e-01,\n",
              "         7.2521e-01,  7.2601e-01,  4.9116e-01,  8.0126e-01,  1.7438e-01,\n",
              "         3.8940e-01,  3.2248e-01,  9.0265e-01, -3.6525e-01,  9.1792e-01,\n",
              "         8.3523e-02,  9.0837e-01, -7.8804e-01,  3.7883e-01, -2.7693e-01,\n",
              "        -7.6169e-02, -3.1370e-01,  5.1356e-02,  8.9001e-01,  8.9951e-01,\n",
              "         8.8598e-01,  5.3285e-01,  9.8173e-01,  9.2909e-01,  9.1511e-01,\n",
              "        -5.7430e-01,  8.1213e-01, -4.6505e-01, -1.4565e-01, -4.9588e-01,\n",
              "        -4.0981e-01,  6.4669e-01,  8.9194e-01,  1.1676e-01, -3.3438e-01,\n",
              "         9.8748e-02,  7.3192e-01,  1.5417e-01, -8.2151e-02,  8.7896e-01,\n",
              "         9.8143e-01, -1.6771e-01, -4.2826e-01,  1.4443e-01,  8.3837e-01,\n",
              "         7.9842e-01,  7.9049e-01, -2.7449e-01,  3.3006e-01,  6.2404e-01,\n",
              "         4.6074e-01,  5.3503e-01,  8.9657e-01,  9.1064e-01,  3.5071e-01,\n",
              "         2.6713e-01,  8.2242e-01,  9.7448e-01,  8.7754e-02, -5.1089e-01,\n",
              "        -2.3151e-02,  6.9179e-01,  5.7130e-01, -4.6508e-01,  7.0106e-01,\n",
              "         7.0686e-01,  1.8730e-01, -9.7456e-01,  7.9399e-01,  4.5850e-02,\n",
              "         1.5597e-01, -1.0615e-01, -6.6197e-01,  3.8303e-02,  1.1386e-01,\n",
              "        -2.4035e-01, -2.3338e-01, -1.5057e-01,  6.8919e-01,  4.4951e-01,\n",
              "         4.2732e-01,  3.7828e-01,  9.9238e-01, -9.6682e-01, -5.9590e-01,\n",
              "        -4.7896e-01,  4.0583e-01,  9.9495e-01,  8.1267e-01,  6.6089e-01,\n",
              "         3.5455e-01,  8.5289e-01,  7.9811e-01,  4.3529e-01,  4.3583e-01,\n",
              "         6.7814e-01, -4.5691e-01,  9.8246e-01, -3.6088e-01,  4.1943e-01,\n",
              "        -1.2094e-01,  4.3392e-01, -8.8244e-01, -3.4519e-01,  8.8082e-01,\n",
              "         4.9159e-01,  7.8884e-01,  8.2900e-01,  4.3949e-01,  9.5606e-01,\n",
              "        -3.4806e-01,  5.2110e-01,  7.7919e-01, -5.7965e-01,  7.4995e-01,\n",
              "         4.0557e-01,  4.4283e-01,  8.2178e-01,  9.1193e-01,  8.2220e-01,\n",
              "        -6.6989e-01, -6.0324e-01,  2.3350e-01,  8.9173e-01, -7.8452e-01,\n",
              "         8.5318e-01,  6.6307e-02,  9.7499e-02,  8.2071e-01,  9.8476e-01,\n",
              "        -2.4337e-02,  3.9667e-01,  4.3598e-03,  8.9350e-01,  5.8648e-01,\n",
              "         6.2242e-01,  8.9723e-01,  8.8163e-01,  6.8262e-01,  6.3941e-01,\n",
              "         4.8614e-01,  6.7936e-01,  4.0507e-01,  4.7869e-01,  8.9605e-01,\n",
              "        -3.0090e-01,  9.2149e-01,  2.8497e-01,  7.4273e-01,  8.7973e-01,\n",
              "         7.3808e-01, -6.1557e-01, -3.7133e-01, -2.3021e-01, -9.1734e-02,\n",
              "         7.8836e-01,  5.0997e-01,  9.1459e-01,  4.4669e-01, -9.0193e-01,\n",
              "         4.8862e-01,  7.1098e-01,  6.2539e-01,  9.0466e-01,  4.8595e-01,\n",
              "         3.1193e-01,  8.9657e-01, -7.8464e-01, -4.1668e-04,  2.4394e-02,\n",
              "         9.4777e-01,  6.1775e-01, -5.0549e-01,  9.0310e-01, -7.9668e-01,\n",
              "        -3.3749e-01,  4.3531e-01, -1.6256e-01,  6.1566e-02,  6.2263e-01,\n",
              "         4.8330e-01, -7.0939e-01,  8.4542e-01, -5.0072e-01,  6.8846e-02,\n",
              "         1.8127e-01,  6.8590e-01, -8.5580e-01,  3.4752e-01,  6.7964e-01,\n",
              "        -9.5373e-02,  5.6859e-01,  9.8559e-01,  7.1057e-01, -3.8532e-01,\n",
              "         6.6254e-01, -1.4383e-01, -9.1734e-02,  1.0037e-01,  4.8123e-01,\n",
              "        -9.5513e-01,  4.4836e-01,  2.7368e-01,  6.1196e-01,  3.3092e-01,\n",
              "         6.9075e-01,  4.6038e-01,  9.0125e-01,  9.7669e-01,  9.5236e-01,\n",
              "        -1.9100e-01, -5.6595e-01,  4.1448e-01,  7.7899e-01, -6.2811e-01,\n",
              "         9.1031e-01,  1.6701e-01,  6.8023e-01,  8.5491e-01,  4.2161e-01,\n",
              "         3.8193e-01,  6.7155e-01, -7.4662e-01,  9.4945e-01, -4.5879e-01,\n",
              "        -2.3754e-01,  4.6603e-01, -5.3351e-02, -7.5970e-01,  5.6807e-01,\n",
              "         8.2817e-01,  1.0137e-01,  8.6875e-01, -1.8319e-01, -3.7450e-01,\n",
              "        -5.9057e-01,  7.3737e-01, -4.6267e-01,  8.7451e-01,  9.8096e-01,\n",
              "        -2.3960e-01,  6.0257e-02,  2.8440e-01,  9.8049e-01,  4.5727e-01,\n",
              "        -1.2626e-01,  6.9446e-01, -2.7045e-01,  7.2452e-02,  9.6204e-01,\n",
              "        -5.1722e-01,  9.6454e-01,  7.9293e-01,  1.9230e-01, -8.0125e-01,\n",
              "        -1.6396e-01,  5.7876e-01,  4.3299e-01,  9.8668e-01,  1.8073e-01,\n",
              "        -9.2314e-01,  8.0760e-01,  8.6273e-01,  6.6475e-01, -7.8012e-01,\n",
              "         9.4691e-01,  1.3031e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = x[y==1][:,0]\n",
        "\n",
        "for b in range(v.shape[0]):\n",
        "    res = 0\n",
        "    for i in range(v.shape[1]):\n",
        "        res += v[b,i] * metric_t[i] * v[b,i]\n",
        "\n",
        "    print(torch.cos(res.abs().sqrt()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84XtsYtADWdW",
        "outputId": "cf86d502-20e3-4e2c-abef-ec29762fef80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4560)\n",
            "tensor(0.3508)\n",
            "tensor(0.7601)\n",
            "tensor(0.9953)\n",
            "tensor(0.7862)\n",
            "tensor(-0.0845)\n",
            "tensor(0.9326)\n",
            "tensor(-0.2928)\n",
            "tensor(-0.8125)\n",
            "tensor(0.1477)\n",
            "tensor(0.5832)\n",
            "tensor(0.4219)\n",
            "tensor(0.7105)\n",
            "tensor(0.1077)\n",
            "tensor(0.0187)\n",
            "tensor(0.8965)\n",
            "tensor(-0.5669)\n",
            "tensor(0.5589)\n",
            "tensor(0.4489)\n",
            "tensor(0.9622)\n",
            "tensor(0.9261)\n",
            "tensor(0.6932)\n",
            "tensor(0.1523)\n",
            "tensor(0.5369)\n",
            "tensor(0.3956)\n",
            "tensor(-0.9318)\n",
            "tensor(0.9215)\n",
            "tensor(0.4832)\n",
            "tensor(0.7432)\n",
            "tensor(0.3703)\n",
            "tensor(-0.5538)\n",
            "tensor(0.6979)\n",
            "tensor(-0.5347)\n",
            "tensor(-0.3733)\n",
            "tensor(0.1622)\n",
            "tensor(0.5428)\n",
            "tensor(0.4442)\n",
            "tensor(0.7134)\n",
            "tensor(-0.5593)\n",
            "tensor(0.7926)\n",
            "tensor(0.4543)\n",
            "tensor(0.5823)\n",
            "tensor(0.8450)\n",
            "tensor(0.5272)\n",
            "tensor(-0.0606)\n",
            "tensor(-0.3137)\n",
            "tensor(0.3291)\n",
            "tensor(-0.9991)\n",
            "tensor(-0.0976)\n",
            "tensor(-0.0103)\n",
            "tensor(0.3893)\n",
            "tensor(0.6863)\n",
            "tensor(0.6448)\n",
            "tensor(0.3152)\n",
            "tensor(-0.0236)\n",
            "tensor(0.5478)\n",
            "tensor(0.7155)\n",
            "tensor(0.2224)\n",
            "tensor(0.6689)\n",
            "tensor(0.8975)\n",
            "tensor(-0.5110)\n",
            "tensor(0.0571)\n",
            "tensor(0.5537)\n",
            "tensor(-0.9974)\n",
            "tensor(-0.0545)\n",
            "tensor(0.9174)\n",
            "tensor(0.6240)\n",
            "tensor(-0.1096)\n",
            "tensor(0.4416)\n",
            "tensor(-0.9889)\n",
            "tensor(-0.0648)\n",
            "tensor(-0.5139)\n",
            "tensor(0.7795)\n",
            "tensor(0.3986)\n",
            "tensor(0.4779)\n",
            "tensor(0.3619)\n",
            "tensor(0.9904)\n",
            "tensor(-0.9997)\n",
            "tensor(0.9744)\n",
            "tensor(0.4802)\n",
            "tensor(0.7385)\n",
            "tensor(0.5559)\n",
            "tensor(0.6829)\n",
            "tensor(0.2252)\n",
            "tensor(-0.3517)\n",
            "tensor(0.1719)\n",
            "tensor(0.2553)\n",
            "tensor(0.8551)\n",
            "tensor(0.8906)\n",
            "tensor(-0.8938)\n",
            "tensor(0.9102)\n",
            "tensor(0.0902)\n",
            "tensor(-0.8155)\n",
            "tensor(0.7647)\n",
            "tensor(-0.9105)\n",
            "tensor(0.7533)\n",
            "tensor(0.8727)\n",
            "tensor(0.7269)\n",
            "tensor(0.9901)\n",
            "tensor(0.9053)\n",
            "tensor(0.8459)\n",
            "tensor(0.9044)\n",
            "tensor(0.9376)\n",
            "tensor(0.3375)\n",
            "tensor(0.4505)\n",
            "tensor(-0.1072)\n",
            "tensor(0.6765)\n",
            "tensor(0.9460)\n",
            "tensor(0.3342)\n",
            "tensor(-0.5295)\n",
            "tensor(0.5108)\n",
            "tensor(0.2479)\n",
            "tensor(-0.0843)\n",
            "tensor(-0.4946)\n",
            "tensor(-0.9172)\n",
            "tensor(0.2131)\n",
            "tensor(-0.9363)\n",
            "tensor(0.4219)\n",
            "tensor(0.0464)\n",
            "tensor(0.0317)\n",
            "tensor(0.9287)\n",
            "tensor(-0.8040)\n",
            "tensor(0.7598)\n",
            "tensor(0.3778)\n",
            "tensor(0.8703)\n",
            "tensor(-0.8317)\n",
            "tensor(0.2244)\n",
            "tensor(-0.9909)\n",
            "tensor(-0.5629)\n",
            "tensor(-0.0769)\n",
            "tensor(0.6726)\n",
            "tensor(0.7249)\n",
            "tensor(-0.2711)\n",
            "tensor(-0.2660)\n",
            "tensor(-0.2308)\n",
            "tensor(0.8999)\n",
            "tensor(0.3213)\n",
            "tensor(0.6879)\n",
            "tensor(-0.2378)\n",
            "tensor(0.2883)\n",
            "tensor(0.8907)\n",
            "tensor(-0.2041)\n",
            "tensor(0.8280)\n",
            "tensor(0.7126)\n",
            "tensor(0.3016)\n",
            "tensor(0.1082)\n",
            "tensor(-0.0057)\n",
            "tensor(0.5716)\n",
            "tensor(0.9854)\n",
            "tensor(-0.7032)\n",
            "tensor(0.1037)\n",
            "tensor(0.9661)\n",
            "tensor(-0.1710)\n",
            "tensor(-0.2738)\n",
            "tensor(0.8248)\n",
            "tensor(0.0561)\n",
            "tensor(-0.0538)\n",
            "tensor(-0.7351)\n",
            "tensor(0.6060)\n",
            "tensor(0.6737)\n",
            "tensor(0.0210)\n",
            "tensor(0.5118)\n",
            "tensor(-0.7668)\n",
            "tensor(-0.0671)\n",
            "tensor(0.8392)\n",
            "tensor(0.4552)\n",
            "tensor(0.7707)\n",
            "tensor(0.6982)\n",
            "tensor(0.4372)\n",
            "tensor(0.7680)\n",
            "tensor(0.6042)\n",
            "tensor(-0.6031)\n",
            "tensor(-0.9817)\n",
            "tensor(0.9710)\n",
            "tensor(0.7199)\n",
            "tensor(0.4275)\n",
            "tensor(0.8093)\n",
            "tensor(0.5145)\n",
            "tensor(-0.3568)\n",
            "tensor(-0.2447)\n",
            "tensor(0.9726)\n",
            "tensor(-0.0737)\n",
            "tensor(0.4571)\n",
            "tensor(0.9374)\n",
            "tensor(-0.1942)\n",
            "tensor(0.5365)\n",
            "tensor(-0.7081)\n",
            "tensor(0.5124)\n",
            "tensor(0.7817)\n",
            "tensor(-0.1500)\n",
            "tensor(0.3497)\n",
            "tensor(0.6634)\n",
            "tensor(0.5935)\n",
            "tensor(-0.0780)\n",
            "tensor(-0.2288)\n",
            "tensor(0.9148)\n",
            "tensor(0.8192)\n",
            "tensor(0.8678)\n",
            "tensor(-0.0346)\n",
            "tensor(-0.9116)\n",
            "tensor(0.3822)\n",
            "tensor(0.1075)\n",
            "tensor(0.8365)\n",
            "tensor(-0.4297)\n",
            "tensor(0.7531)\n",
            "tensor(0.7836)\n",
            "tensor(0.4056)\n",
            "tensor(-0.8020)\n",
            "tensor(0.5582)\n",
            "tensor(0.9136)\n",
            "tensor(0.3169)\n",
            "tensor(0.6952)\n",
            "tensor(0.5136)\n",
            "tensor(-0.7092)\n",
            "tensor(0.4068)\n",
            "tensor(0.8504)\n",
            "tensor(0.6996)\n",
            "tensor(0.2722)\n",
            "tensor(0.2634)\n",
            "tensor(-0.6974)\n",
            "tensor(-0.9329)\n",
            "tensor(0.9787)\n",
            "tensor(0.1345)\n",
            "tensor(-0.1210)\n",
            "tensor(0.6425)\n",
            "tensor(0.2384)\n",
            "tensor(0.7854)\n",
            "tensor(-0.5112)\n",
            "tensor(-0.4070)\n",
            "tensor(0.5426)\n",
            "tensor(0.0455)\n",
            "tensor(0.8592)\n",
            "tensor(-0.1545)\n",
            "tensor(0.4591)\n",
            "tensor(0.1424)\n",
            "tensor(0.6268)\n",
            "tensor(-0.4338)\n",
            "tensor(-0.3348)\n",
            "tensor(-0.4482)\n",
            "tensor(-0.3875)\n",
            "tensor(-0.1002)\n",
            "tensor(0.7428)\n",
            "tensor(0.7779)\n",
            "tensor(0.7892)\n",
            "tensor(-0.1771)\n",
            "tensor(-0.7378)\n",
            "tensor(-0.9224)\n",
            "tensor(0.8906)\n",
            "tensor(0.8959)\n",
            "tensor(-0.3264)\n",
            "tensor(-0.0891)\n",
            "tensor(0.2738)\n",
            "tensor(0.6687)\n",
            "tensor(0.9911)\n",
            "tensor(0.4941)\n",
            "tensor(0.7252)\n",
            "tensor(0.7260)\n",
            "tensor(0.4912)\n",
            "tensor(0.8013)\n",
            "tensor(0.1744)\n",
            "tensor(0.3894)\n",
            "tensor(0.3225)\n",
            "tensor(0.9026)\n",
            "tensor(-0.3653)\n",
            "tensor(0.9179)\n",
            "tensor(0.0835)\n",
            "tensor(0.9084)\n",
            "tensor(-0.7880)\n",
            "tensor(0.3788)\n",
            "tensor(-0.2769)\n",
            "tensor(-0.0762)\n",
            "tensor(-0.3137)\n",
            "tensor(0.0514)\n",
            "tensor(0.8900)\n",
            "tensor(0.8995)\n",
            "tensor(0.8860)\n",
            "tensor(0.5329)\n",
            "tensor(0.9817)\n",
            "tensor(0.9291)\n",
            "tensor(0.9151)\n",
            "tensor(-0.5743)\n",
            "tensor(0.8121)\n",
            "tensor(-0.4650)\n",
            "tensor(-0.1457)\n",
            "tensor(-0.4959)\n",
            "tensor(-0.4098)\n",
            "tensor(0.6467)\n",
            "tensor(0.8919)\n",
            "tensor(0.1168)\n",
            "tensor(-0.3344)\n",
            "tensor(0.0987)\n",
            "tensor(0.7319)\n",
            "tensor(0.1542)\n",
            "tensor(-0.0822)\n",
            "tensor(0.8790)\n",
            "tensor(0.9814)\n",
            "tensor(-0.1677)\n",
            "tensor(-0.4283)\n",
            "tensor(0.1444)\n",
            "tensor(0.8384)\n",
            "tensor(0.7984)\n",
            "tensor(0.7905)\n",
            "tensor(-0.2745)\n",
            "tensor(0.3301)\n",
            "tensor(0.6240)\n",
            "tensor(0.4607)\n",
            "tensor(0.5350)\n",
            "tensor(0.8966)\n",
            "tensor(0.9106)\n",
            "tensor(0.3507)\n",
            "tensor(0.2671)\n",
            "tensor(0.8224)\n",
            "tensor(0.9745)\n",
            "tensor(0.0878)\n",
            "tensor(-0.5109)\n",
            "tensor(-0.0232)\n",
            "tensor(0.6918)\n",
            "tensor(0.5713)\n",
            "tensor(-0.4651)\n",
            "tensor(0.7011)\n",
            "tensor(0.7069)\n",
            "tensor(0.1873)\n",
            "tensor(-0.9746)\n",
            "tensor(0.7940)\n",
            "tensor(0.0458)\n",
            "tensor(0.1560)\n",
            "tensor(-0.1062)\n",
            "tensor(-0.6620)\n",
            "tensor(0.0383)\n",
            "tensor(0.1139)\n",
            "tensor(-0.2403)\n",
            "tensor(-0.2334)\n",
            "tensor(-0.1506)\n",
            "tensor(0.6892)\n",
            "tensor(0.4495)\n",
            "tensor(0.4273)\n",
            "tensor(0.3783)\n",
            "tensor(0.9924)\n",
            "tensor(-0.9668)\n",
            "tensor(-0.5959)\n",
            "tensor(-0.4790)\n",
            "tensor(0.4058)\n",
            "tensor(0.9950)\n",
            "tensor(0.8127)\n",
            "tensor(0.6609)\n",
            "tensor(0.3545)\n",
            "tensor(0.8529)\n",
            "tensor(0.7981)\n",
            "tensor(0.4353)\n",
            "tensor(0.4358)\n",
            "tensor(0.6781)\n",
            "tensor(-0.4569)\n",
            "tensor(0.9825)\n",
            "tensor(-0.3609)\n",
            "tensor(0.4194)\n",
            "tensor(-0.1209)\n",
            "tensor(0.4339)\n",
            "tensor(-0.8824)\n",
            "tensor(-0.3452)\n",
            "tensor(0.8808)\n",
            "tensor(0.4916)\n",
            "tensor(0.7888)\n",
            "tensor(0.8290)\n",
            "tensor(0.4395)\n",
            "tensor(0.9561)\n",
            "tensor(-0.3481)\n",
            "tensor(0.5211)\n",
            "tensor(0.7792)\n",
            "tensor(-0.5797)\n",
            "tensor(0.7499)\n",
            "tensor(0.4056)\n",
            "tensor(0.4428)\n",
            "tensor(0.8218)\n",
            "tensor(0.9119)\n",
            "tensor(0.8222)\n",
            "tensor(-0.6699)\n",
            "tensor(-0.6032)\n",
            "tensor(0.2335)\n",
            "tensor(0.8917)\n",
            "tensor(-0.7845)\n",
            "tensor(0.8532)\n",
            "tensor(0.0663)\n",
            "tensor(0.0975)\n",
            "tensor(0.8207)\n",
            "tensor(0.9848)\n",
            "tensor(-0.0243)\n",
            "tensor(0.3967)\n",
            "tensor(0.0044)\n",
            "tensor(0.8935)\n",
            "tensor(0.5865)\n",
            "tensor(0.6224)\n",
            "tensor(0.8972)\n",
            "tensor(0.8816)\n",
            "tensor(0.6826)\n",
            "tensor(0.6394)\n",
            "tensor(0.4861)\n",
            "tensor(0.6794)\n",
            "tensor(0.4051)\n",
            "tensor(0.4787)\n",
            "tensor(0.8960)\n",
            "tensor(-0.3009)\n",
            "tensor(0.9215)\n",
            "tensor(0.2850)\n",
            "tensor(0.7427)\n",
            "tensor(0.8797)\n",
            "tensor(0.7381)\n",
            "tensor(-0.6156)\n",
            "tensor(-0.3713)\n",
            "tensor(-0.2302)\n",
            "tensor(-0.0917)\n",
            "tensor(0.7884)\n",
            "tensor(0.5100)\n",
            "tensor(0.9146)\n",
            "tensor(0.4467)\n",
            "tensor(-0.9019)\n",
            "tensor(0.4886)\n",
            "tensor(0.7110)\n",
            "tensor(0.6254)\n",
            "tensor(0.9047)\n",
            "tensor(0.4859)\n",
            "tensor(0.3119)\n",
            "tensor(0.8966)\n",
            "tensor(-0.7846)\n",
            "tensor(-0.0004)\n",
            "tensor(0.0244)\n",
            "tensor(0.9478)\n",
            "tensor(0.6178)\n",
            "tensor(-0.5055)\n",
            "tensor(0.9031)\n",
            "tensor(-0.7967)\n",
            "tensor(-0.3375)\n",
            "tensor(0.4353)\n",
            "tensor(-0.1626)\n",
            "tensor(0.0616)\n",
            "tensor(0.6226)\n",
            "tensor(0.4833)\n",
            "tensor(-0.7094)\n",
            "tensor(0.8454)\n",
            "tensor(-0.5007)\n",
            "tensor(0.0688)\n",
            "tensor(0.1813)\n",
            "tensor(0.6859)\n",
            "tensor(-0.8558)\n",
            "tensor(0.3475)\n",
            "tensor(0.6796)\n",
            "tensor(-0.0954)\n",
            "tensor(0.5686)\n",
            "tensor(0.9856)\n",
            "tensor(0.7106)\n",
            "tensor(-0.3853)\n",
            "tensor(0.6625)\n",
            "tensor(-0.1438)\n",
            "tensor(-0.0917)\n",
            "tensor(0.1004)\n",
            "tensor(0.4812)\n",
            "tensor(-0.9551)\n",
            "tensor(0.4484)\n",
            "tensor(0.2737)\n",
            "tensor(0.6120)\n",
            "tensor(0.3309)\n",
            "tensor(0.6908)\n",
            "tensor(0.4604)\n",
            "tensor(0.9013)\n",
            "tensor(0.9767)\n",
            "tensor(0.9524)\n",
            "tensor(-0.1910)\n",
            "tensor(-0.5660)\n",
            "tensor(0.4145)\n",
            "tensor(0.7790)\n",
            "tensor(-0.6281)\n",
            "tensor(0.9103)\n",
            "tensor(0.1670)\n",
            "tensor(0.6802)\n",
            "tensor(0.8549)\n",
            "tensor(0.4216)\n",
            "tensor(0.3819)\n",
            "tensor(0.6716)\n",
            "tensor(-0.7466)\n",
            "tensor(0.9494)\n",
            "tensor(-0.4588)\n",
            "tensor(-0.2375)\n",
            "tensor(0.4660)\n",
            "tensor(-0.0534)\n",
            "tensor(-0.7597)\n",
            "tensor(0.5681)\n",
            "tensor(0.8282)\n",
            "tensor(0.1014)\n",
            "tensor(0.8688)\n",
            "tensor(-0.1832)\n",
            "tensor(-0.3745)\n",
            "tensor(-0.5906)\n",
            "tensor(0.7374)\n",
            "tensor(-0.4627)\n",
            "tensor(0.8745)\n",
            "tensor(0.9810)\n",
            "tensor(-0.2396)\n",
            "tensor(0.0603)\n",
            "tensor(0.2844)\n",
            "tensor(0.9805)\n",
            "tensor(0.4573)\n",
            "tensor(-0.1263)\n",
            "tensor(0.6945)\n",
            "tensor(-0.2705)\n",
            "tensor(0.0725)\n",
            "tensor(0.9620)\n",
            "tensor(-0.5172)\n",
            "tensor(0.9645)\n",
            "tensor(0.7929)\n",
            "tensor(0.1923)\n",
            "tensor(-0.8013)\n",
            "tensor(-0.1640)\n",
            "tensor(0.5788)\n",
            "tensor(0.4330)\n",
            "tensor(0.9867)\n",
            "tensor(0.1807)\n",
            "tensor(-0.9231)\n",
            "tensor(0.8076)\n",
            "tensor(0.8627)\n",
            "tensor(0.6647)\n",
            "tensor(-0.7801)\n",
            "tensor(0.9469)\n",
            "tensor(0.1303)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cliffor Algebra Operations"
      ],
      "metadata": {
        "id": "W2AQTxTayePr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CliffordAlgebra(nn.Module):\n",
        "    def __init__(self, metric):\n",
        "        super().__init__()\n",
        "\n",
        "        self.register_buffer(\"metric\", torch.as_tensor(metric))\n",
        "        self.num_bases = len(metric)\n",
        "        self.bbo = ShortLexBasisBladeOrder(self.num_bases)\n",
        "        self.dim = len(self.metric)\n",
        "        self.n_blades = len(self.bbo.grades)\n",
        "        cayley = (\n",
        "            construct_gmt(\n",
        "                self.bbo.index_to_bitmap, self.bbo.bitmap_to_index, self.metric\n",
        "            )\n",
        "            .to_dense()\n",
        "            .to(torch.get_default_dtype())\n",
        "        )\n",
        "        self.grades = self.bbo.grades.unique()\n",
        "        self.register_buffer(\n",
        "            \"subspaces\",\n",
        "            torch.tensor(tuple(math.comb(self.dim, g) for g in self.grades)),\n",
        "        )\n",
        "        self.n_subspaces = len(self.grades)\n",
        "        self.grade_to_slice = self._grade_to_slice(self.subspaces)\n",
        "        self.grade_to_index = [\n",
        "            torch.tensor(range(*s.indices(s.stop))) for s in self.grade_to_slice\n",
        "        ]\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"bbo_grades\", self.bbo.grades.to(torch.get_default_dtype())\n",
        "        )\n",
        "        self.register_buffer(\"even_grades\", self.bbo_grades % 2 == 0)\n",
        "        self.register_buffer(\"odd_grades\", ~self.even_grades)\n",
        "        self.register_buffer(\"cayley\", cayley)\n",
        "\n",
        "    def geometric_product(self, a, b, blades=None):\n",
        "        cayley = self.cayley\n",
        "\n",
        "        if blades is not None:\n",
        "            blades_l, blades_o, blades_r = blades\n",
        "            assert isinstance(blades_l, torch.Tensor)\n",
        "            assert isinstance(blades_o, torch.Tensor)\n",
        "            assert isinstance(blades_r, torch.Tensor)\n",
        "            cayley = cayley[blades_l[:, None, None], blades_o[:, None], blades_r]\n",
        "\n",
        "        return torch.einsum(\"...i,ijk,...k->...j\", a, cayley, b)\n",
        "\n",
        "    def _grade_to_slice(self, subspaces):\n",
        "        grade_to_slice = list()\n",
        "        subspaces = torch.as_tensor(subspaces)\n",
        "        for grade in self.grades:\n",
        "            index_start = subspaces[:grade].sum()\n",
        "            index_end = index_start + math.comb(self.dim, grade)\n",
        "            grade_to_slice.append(slice(index_start, index_end))\n",
        "        return grade_to_slice\n",
        "\n",
        "    @functools.cached_property\n",
        "    def _alpha_signs(self):\n",
        "        return torch.pow(-1, self.bbo_grades)\n",
        "\n",
        "    @functools.cached_property\n",
        "    def _beta_signs(self):\n",
        "        return torch.pow(-1, self.bbo_grades * (self.bbo_grades - 1) // 2)\n",
        "\n",
        "    @functools.cached_property\n",
        "    def _gamma_signs(self):\n",
        "        return torch.pow(-1, self.bbo_grades * (self.bbo_grades + 1) // 2)\n",
        "\n",
        "    def alpha(self, mv, blades=None):\n",
        "        signs = self._alpha_signs\n",
        "        if blades is not None:\n",
        "            signs = signs[blades]\n",
        "        return signs * mv.clone()\n",
        "\n",
        "    def beta(self, mv, blades=None):\n",
        "        signs = self._beta_signs\n",
        "        if blades is not None:\n",
        "            signs = signs[blades]\n",
        "        return signs * mv.clone()\n",
        "\n",
        "    def gamma(self, mv, blades=None):\n",
        "        signs = self._gamma_signs\n",
        "        if blades is not None:\n",
        "            signs = signs[blades]\n",
        "        return signs * mv.clone()\n",
        "\n",
        "    def zeta(self, mv):\n",
        "        return mv[..., :1]\n",
        "\n",
        "    def embed(self, tensor: torch.Tensor, tensor_index: torch.Tensor) -> torch.Tensor:\n",
        "        mv = torch.zeros(\n",
        "            *tensor.shape[:-1], 2**self.dim, device=tensor.device, dtype=tensor.dtype\n",
        "        )\n",
        "        mv[..., tensor_index] = tensor\n",
        "        return mv\n",
        "\n",
        "    def embed_grade(self, tensor: torch.Tensor, grade: int) -> torch.Tensor:\n",
        "        mv = torch.zeros(*tensor.shape[:-1], 2**self.dim, device=tensor.device)\n",
        "        s = self.grade_to_slice[grade]\n",
        "        mv[..., s] = tensor\n",
        "        return mv\n",
        "\n",
        "    def get(self, mv: torch.Tensor, blade_index: tuple[int]) -> torch.Tensor:\n",
        "        blade_index = tuple(blade_index)\n",
        "        return mv[..., blade_index]\n",
        "\n",
        "    def get_grade(self, mv: torch.Tensor, grade: int) -> torch.Tensor:\n",
        "        s = self.grade_to_slice[grade]\n",
        "        return mv[..., s]\n",
        "\n",
        "    def b(self, x, y, blades=None):\n",
        "        if blades is not None:\n",
        "            assert len(blades) == 2\n",
        "            beta_blades = blades[0]\n",
        "            blades = (\n",
        "                blades[0],\n",
        "                torch.tensor([0]),\n",
        "                blades[1],\n",
        "            )\n",
        "        else:\n",
        "            blades = torch.tensor(range(self.n_blades))\n",
        "            blades = (\n",
        "                blades,\n",
        "                torch.tensor([0]),\n",
        "                blades,\n",
        "            )\n",
        "            beta_blades = None\n",
        "\n",
        "        return self.geometric_product(\n",
        "            self.beta(x, blades=beta_blades),\n",
        "            y,\n",
        "            blades=blades,\n",
        "        )\n",
        "\n",
        "    def q(self, mv, blades=None):\n",
        "        if blades is not None:\n",
        "            blades = (blades, blades)\n",
        "        return self.b(mv, mv, blades=blades)\n",
        "\n",
        "    def _smooth_abs_sqrt(self, input, eps=1e-16):\n",
        "        return (input**2 + eps) ** 0.25\n",
        "\n",
        "    def norm(self, mv, blades=None):\n",
        "        return self._smooth_abs_sqrt(self.q(mv, blades=blades))\n",
        "\n",
        "    def norms(self, mv, grades=None):\n",
        "        if grades is None:\n",
        "            grades = self.grades\n",
        "        return [\n",
        "            self.norm(self.get_grade(mv, grade), blades=self.grade_to_index[grade])\n",
        "            for grade in grades\n",
        "        ]\n",
        "\n",
        "    def qs(self, mv, grades=None):\n",
        "        if grades is None:\n",
        "            grades = self.grades\n",
        "        return [\n",
        "            self.q(self.get_grade(mv, grade), blades=self.grade_to_index[grade])\n",
        "            for grade in grades\n",
        "        ]\n",
        "\n",
        "    def sandwich(self, u, v, w):\n",
        "        return self.geometric_product(self.geometric_product(u, v), w)\n",
        "\n",
        "    def output_blades(self, blades_left, blades_right):\n",
        "        blades = []\n",
        "        for blade_left in blades_left:\n",
        "            for blade_right in blades_right:\n",
        "                bitmap_left = self.bbo.index_to_bitmap[blade_left]\n",
        "                bitmap_right = self.bbo.index_to_bitmap[blade_right]\n",
        "                bitmap_out, _ = gmt_element(bitmap_left, bitmap_right, self.metric)\n",
        "                index_out = self.bbo.bitmap_to_index[bitmap_out]\n",
        "                blades.append(index_out)\n",
        "\n",
        "        return torch.tensor(blades)\n",
        "\n",
        "    def random(self, n=None):\n",
        "        if n is None:\n",
        "            n = 1\n",
        "        return torch.randn(n, self.n_blades)\n",
        "\n",
        "    def random_vector(self, n=None):\n",
        "        if n is None:\n",
        "            n = 1\n",
        "        vector_indices = self.bbo_grades == 1\n",
        "        v = torch.zeros(n, self.n_blades, device=self.cayley.device)\n",
        "        v[:, vector_indices] = torch.randn(\n",
        "            n, vector_indices.sum(), device=self.cayley.device\n",
        "        )\n",
        "        return v\n",
        "\n",
        "    def parity(self, mv):\n",
        "        is_odd = torch.all(mv[..., self.even_grades] == 0)\n",
        "        is_even = torch.all(mv[..., self.odd_grades] == 0)\n",
        "\n",
        "        if is_odd ^ is_even:  # exclusive or (xor)\n",
        "            return is_odd\n",
        "        else:\n",
        "            raise ValueError(\"This is not a homogeneous element.\")\n",
        "\n",
        "    def eta(self, w):\n",
        "        return (-1) ** self.parity(w)\n",
        "\n",
        "    def alpha_w(self, w, mv):\n",
        "        return self.even_grades * mv + self.eta(w) * self.odd_grades * mv\n",
        "\n",
        "    def inverse(self, mv, blades=None):\n",
        "        mv_ = self.beta(mv, blades=blades)\n",
        "        return mv_ / self.q(mv)\n",
        "\n",
        "    def rho(self, w, mv):\n",
        "        \"\"\"Applies the versor w action to mv.\"\"\"\n",
        "        return self.sandwich(w, self.alpha_w(w, mv), self.inverse(w))\n",
        "\n",
        "    def reduce_geometric_product(self, inputs):\n",
        "        return functools.reduce(self.geometric_product, inputs)\n",
        "\n",
        "    def versor(self, order=None, normalized=True):\n",
        "        if order is None:\n",
        "            order = self.dim if self.dim % 2 == 0 else self.dim - 1\n",
        "        vectors = self.random_vector(order)\n",
        "        versor = self.reduce_geometric_product(vectors[:, None])\n",
        "        if normalized:\n",
        "            versor = versor / self.norm(versor)[..., :1]\n",
        "        return versor\n",
        "\n",
        "    def rotor(self):\n",
        "        return self.versor()\n",
        "\n",
        "    @functools.cached_property\n",
        "    def geometric_product_paths(self):\n",
        "        gp_paths = torch.zeros((self.dim + 1, self.dim + 1, self.dim + 1), dtype=bool)\n",
        "\n",
        "        for i in range(self.dim + 1):\n",
        "            for j in range(self.dim + 1):\n",
        "                for k in range(self.dim + 1):\n",
        "                    s_i = self.grade_to_slice[i]\n",
        "                    s_j = self.grade_to_slice[j]\n",
        "                    s_k = self.grade_to_slice[k]\n",
        "\n",
        "                    m = self.cayley[s_i, s_j, s_k]\n",
        "                    gp_paths[i, j, k] = (m != 0).any()\n",
        "\n",
        "        return gp_paths"
      ],
      "metadata": {
        "id": "iKveBp-yyhoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom CG Layers"
      ],
      "metadata": {
        "id": "kbkI0FdAqCXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Linear Layer\n",
        "\n",
        "class MVLinear(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        algebra,\n",
        "        in_features,\n",
        "        out_features,\n",
        "        subspaces=True,\n",
        "        bias=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.algebra = algebra\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.subspaces = subspaces\n",
        "\n",
        "        if subspaces:\n",
        "            self.weight = nn.Parameter(\n",
        "                torch.empty(out_features, in_features, algebra.n_subspaces)\n",
        "            )\n",
        "            self._forward = self._forward_subspaces\n",
        "        else:\n",
        "            self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.empty(1, out_features, 1))\n",
        "            self.b_dims = (0,)\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "            self.b_dims = ()\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.normal_(self.weight, std=1 / math.sqrt(self.in_features))\n",
        "\n",
        "        if self.bias is not None:\n",
        "            torch.nn.init.zeros_(self.bias)\n",
        "\n",
        "    def _forward(self, input):\n",
        "        return torch.einsum(\"bm...i, nm->bn...i\", input, self.weight)\n",
        "\n",
        "    def _forward_subspaces(self, input):\n",
        "        weight = self.weight.repeat_interleave(self.algebra.subspaces, dim=-1)\n",
        "        return torch.einsum(\"bm...i, nmi->bn...i\", input, weight)\n",
        "\n",
        "    def forward(self, input):\n",
        "        result = self._forward(input)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            bias = self.algebra.embed(self.bias, self.b_dims)\n",
        "            result += unsqueeze_like(bias, result, dim=2)\n",
        "        return result"
      ],
      "metadata": {
        "id": "zmPXtT6nqFgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu8Fj6_mCVfA"
      },
      "source": [
        "# Model\n",
        "Now that we created the inputs and targets for our model to learn, we specify the model. To do so, we first import the necessary objects, and create the `CliffordAlgebra` object which takes as input the metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3AE9xlElgUh"
      },
      "outputs": [],
      "source": [
        "from models.modules.linear import MVLinear\n",
        "from models.modules.gp import SteerableGeometricProductLayer\n",
        "from models.modules.mvlayernorm import MVLayerNorm\n",
        "from models.modules.mvsilu import MVSiLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwKegYP7Ca4m"
      },
      "source": [
        "Let's create a Clifford Group Equivariant model block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGLc-5a5l88p"
      },
      "outputs": [],
      "source": [
        "class CGEBlock(nn.Module):\n",
        "    def __init__(self, algebra, in_features, out_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            MVLinear(algebra, in_features, out_features),\n",
        "            MVSiLU(algebra, out_features),\n",
        "            SteerableGeometricProductLayer(algebra, out_features),\n",
        "            MVLayerNorm(algebra, out_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        # [batch_size, in_features, 2**d] -> [batch_size, out_features, 2**d]\n",
        "        return self.layers(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKNuOr-zCest"
      },
      "source": [
        "This is a feedforward transformation, going from `in_features` to `out_features` (similar to the usual neural network setting) using a few modules.\n",
        "\n",
        "First, we apply a linear transformation (`MVLinear`) followed by a nonlinear `MVSiLU` layer, a multivector analogue of the `SiLU` or *swish* activation. Other equivariant nonlinearities are also possible, but have not been implemented yet.\n",
        "\n",
        "Then we apply a `SteerableGeometricProductLayer`, which computes geometric products between our input data in a manner similar to *self-attention* (see the implementation of the block for more details).\n",
        "\n",
        "Finally, we apply a multivector analogue of LayerNorm: `MVLayerNorm`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY31YxJkm6_c"
      },
      "outputs": [],
      "source": [
        "class CGEMLP(nn.Module):\n",
        "    def __init__(self, algebra, in_features, hidden_features, out_features, n_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        for i in range(n_layers - 1):\n",
        "            layers.append(\n",
        "                CGEBlock(algebra, in_features, hidden_features)\n",
        "            )\n",
        "            in_features = hidden_features\n",
        "\n",
        "        layers.append(\n",
        "            CGEBlock(algebra, hidden_features, out_features)\n",
        "        )\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.layers(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sn3aFDDCmDl"
      },
      "source": [
        "We simply chain the `CGEBlocks` to get an expressive architecture.\n",
        "\n",
        "Finally, we specify our final model class, which we call `InvariantCGENN`. Since we are approximating an $O(n)$-*invariant* function, we can do some invariant post-processing using a regular `MLP`.\n",
        "\n",
        "This is shown in the `forward` function. We first process the input through the `CGEMLP`, which will output *multivector*-valued hidden states. The *grade-0* (scalar) subspace of a multivector is always *invariant*. Hence, we index the last dimension of the hidden states at 0 and let an `MLP` process this. The final output remains invariant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQv7crZbra4t"
      },
      "outputs": [],
      "source": [
        "class InvariantCGENN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, hidden_features, out_features):\n",
        "        super().__init__()\n",
        "        self.cgemlp = CGEMLP(ca, in_features, hidden_features, hidden_features)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_features, hidden_features),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_features, hidden_features),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_features, out_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        h = self.cgemlp(input)\n",
        "        # Index the hidden states at 0 to get the invariants, and let a regular MLP do the final processing.\n",
        "        return self.mlp(h[..., 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aYVVVluCqF3"
      },
      "source": [
        "# Data Embedding\n",
        "\n",
        "Now we have to prepare the data to be processed by the network.\n",
        "\n",
        "To do so, we first create the `CliffordAlgebra` object, providing us with the necessary Clifford algebra operations, such as the *geometric product*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23op_HikkOXJ",
        "outputId": "fbfb1e0b-f156-4ab1-99ed-187d75796263"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CliffordAlgebra()"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "from algebra.cliffordalgebra import CliffordAlgebra\n",
        "ca = CliffordAlgebra(metric)\n",
        "ca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pukVHmjPllrx"
      },
      "source": [
        "Then, recall that the two vector features $u, v \\in \\mathbb{R}^n$. The *first* Clifford subspace (or grade 1) is the vector subspace. We also write $\\mathrm{Cl}^{(1)}(V, q) = \\mathbb{R}^n$.\n",
        "\n",
        "Therefore:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is_U0kKXCukD",
        "outputId": "52a055f9-2aa0-40a3-844c-ca6a991b1775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024, 2, 4])\n",
            "tensor([[ 0.0000, -1.6215,  0.4577,  0.0000],\n",
            "        [ 0.0000, -1.1135,  0.6075,  0.0000]])\n"
          ]
        }
      ],
      "source": [
        "x_cl = ca.embed_grade(x, 1)\n",
        "print(x_cl.shape)\n",
        "print(x_cl[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JusqG96_Cyar"
      },
      "source": [
        "Note that the final shape increased to $2^d$. That's because we added the other Clifford subspaces (they are set to zero initially).\n",
        "\n",
        "\n",
        "Next, we *one-hot* encode the categorical variable $y$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eWl1XbJCwzS",
        "outputId": "beb4f585-8e43-40a4-e726-fcaf1c5bbd25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        ...,\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "y_oh = F.one_hot(y - 1, 2)\n",
        "y_oh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z84Drt3C4Ye"
      },
      "source": [
        "This categorical variable can be regarded as a scalar (invariant). Hence, we embed it in the zero grade, i.e., $\\mathrm{Cl}^{(0)}(V, q)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rLtLHFHC2bS",
        "outputId": "d4bf644f-0332-4945-f296-1f94bd27cc4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024, 2, 4])\n",
            "tensor([[0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "y_cl = ca.embed_grade(y_oh[..., None], 0)\n",
        "print(y_cl.shape)\n",
        "print(y_cl[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66NRNCOC-Wk"
      },
      "source": [
        "Note that only the scalar part has a 0 or 1 entry.\n",
        "\n",
        "We now concatenate to get our model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubn7fr_6C8n0"
      },
      "outputs": [],
      "source": [
        "input_cl = torch.cat([x_cl, y_cl], dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_P3TjqXDA82"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now we can train our model using regular backpropagation. We specify the input features, hidden features nad output features and use the Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMyfzlyGC_fQ",
        "outputId": "1d8e71c9-a131-4e0e-ed08-1ed693e4be55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 19297 parameters.\n",
            "\n",
            "Step: 0. Loss: 1.09\n",
            "Step: 4. Loss: 1.01\n",
            "Step: 8. Loss: 0.94\n",
            "Step: 12. Loss: 0.87\n",
            "Step: 16. Loss: 0.80\n",
            "Step: 20. Loss: 0.74\n",
            "Step: 24. Loss: 0.67\n",
            "Step: 28. Loss: 0.60\n",
            "Step: 32. Loss: 0.53\n",
            "Step: 36. Loss: 0.45\n",
            "Step: 40. Loss: 0.37\n",
            "Step: 44. Loss: 0.30\n",
            "Step: 48. Loss: 0.24\n",
            "Step: 52. Loss: 0.19\n",
            "Step: 56. Loss: 0.14\n",
            "Step: 60. Loss: 0.10\n",
            "Step: 64. Loss: 0.07\n",
            "Step: 68. Loss: 0.04\n",
            "Step: 72. Loss: 0.03\n",
            "Step: 76. Loss: 0.02\n",
            "Step: 80. Loss: 0.01\n",
            "Step: 84. Loss: 0.01\n",
            "Step: 88. Loss: 0.01\n",
            "Step: 92. Loss: 0.01\n",
            "Step: 96. Loss: 0.01\n",
            "Step: 100. Loss: 0.00\n",
            "Step: 104. Loss: 0.00\n",
            "Step: 108. Loss: 0.00\n",
            "Step: 112. Loss: 0.00\n",
            "Step: 116. Loss: 0.00\n",
            "Step: 120. Loss: 0.00\n",
            "Step: 124. Loss: 0.00\n",
            "Step: 128. Loss: 0.00\n",
            "Step: 132. Loss: 0.00\n",
            "Step: 136. Loss: 0.00\n",
            "Step: 140. Loss: 0.00\n",
            "Step: 144. Loss: 0.00\n",
            "Step: 148. Loss: 0.00\n",
            "Step: 152. Loss: 0.00\n",
            "Step: 156. Loss: 0.00\n",
            "Step: 160. Loss: 0.00\n",
            "Step: 164. Loss: 0.00\n",
            "Step: 168. Loss: 0.00\n",
            "Step: 172. Loss: 0.00\n",
            "Step: 176. Loss: 0.00\n",
            "Step: 180. Loss: 0.00\n",
            "Step: 184. Loss: 0.00\n",
            "Step: 188. Loss: 0.00\n",
            "Step: 192. Loss: 0.00\n",
            "Step: 196. Loss: 0.00\n",
            "Step: 200. Loss: 0.00\n",
            "Step: 204. Loss: 0.00\n",
            "Step: 208. Loss: 0.00\n",
            "Step: 212. Loss: 0.00\n",
            "Step: 216. Loss: 0.00\n",
            "Step: 220. Loss: 0.00\n",
            "Step: 224. Loss: 0.00\n",
            "Step: 228. Loss: 0.00\n",
            "Step: 232. Loss: 0.00\n",
            "Step: 236. Loss: 0.00\n",
            "Step: 240. Loss: 0.00\n",
            "Step: 244. Loss: 0.00\n",
            "Step: 248. Loss: 0.00\n",
            "Step: 252. Loss: 0.00\n"
          ]
        }
      ],
      "source": [
        "model = InvariantCGENN(4, 32, 1)\n",
        "\n",
        "print(f\"The model has {sum(p.numel() for p in model.parameters())} parameters.\\n\")\n",
        "adam = optim.Adam(model.parameters())\n",
        "\n",
        "for i in range(256):\n",
        "\n",
        "    output = model(input_cl)\n",
        "    loss = F.mse_loss(output.squeeze(-1), f)\n",
        "\n",
        "    adam.zero_grad()\n",
        "    loss.backward()\n",
        "    adam.step()\n",
        "\n",
        "    if i % 4 == 0:\n",
        "        print(f\"Step: {i}. Loss: {loss.item():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPtlVGoxDNru"
      },
      "source": [
        "# Equivariance Assessment\n",
        "\n",
        "Let's get a rotated version of our inputs! We can use the Clifford algebra to sample a random rotation. This is done by sampling a *versor*. A $k$-versor parameterizes an orthogonal transformation. For $k=1$ we have a reflection, for $k=2$ a rotation, for $k=3$ a rotoreflection, and so on.\n",
        "\n",
        "To do a good equivariance assessment, we need to take care of numerical precision issues which are solved by going to `torch.float64`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx9RYGmKDC9z"
      },
      "outputs": [],
      "source": [
        "torch.set_default_dtype(torch.float64)\n",
        "ca = ca.double()\n",
        "model = model.double()\n",
        "input_cl = input_cl.double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Woq4lcU6DP0E"
      },
      "outputs": [],
      "source": [
        "w = ca.versor(2)  # A rotation can be obtained by composing two reflections, i.e., a 2-versor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjvTP22uDTLp"
      },
      "source": [
        "We apply the group action $\\rho(w): \\mathrm{Cl}(V, q) \\to \\mathrm{Cl}(V, q)$ to the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYEbBlzEDQhy"
      },
      "outputs": [],
      "source": [
        "input_cl_w = ca.rho(w, input_cl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS-ISckiDXAL"
      },
      "source": [
        "We first start with the hidden output states of the Clifford MLP, which are not invariant, but *equivariant*. That is, they should *corotate* with our input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQueKwOtDSBy"
      },
      "outputs": [],
      "source": [
        "h_cemlp = model.cgemlp(input_cl)\n",
        "h_w_cemlp = model.cgemlp(input_cl_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0vAip53DX7W"
      },
      "outputs": [],
      "source": [
        "assert torch.allclose(ca.rho(w, h_cemlp), h_w_cemlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJH9KouDZpw"
      },
      "source": [
        "The equivariance checks out! Note that these hidden representations are *multivectors*. I.e., they have the following shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r8xHH2TDZZn",
        "outputId": "9c9bce3c-d8b9-4e61-cbb1-a986dddaeaff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024, 32, 4])\n",
            "tensor([ 1.6597,  0.1700, -0.0447, -0.0433], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(h_cemlp.shape)  # [batch size, hidden_features, 2 ** d]\n",
        "print(h_cemlp[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5dOiVC9Db2r"
      },
      "source": [
        "Now we investigate if the final prediction really is invariant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkcnBui_DYs_"
      },
      "outputs": [],
      "source": [
        "assert torch.allclose(model(input_cl), model(input_cl_w))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ai_RwaPDdww"
      },
      "source": [
        "üòç"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bI3msjZDfdO"
      },
      "source": [
        "# Closing Remarks\n",
        "**Try running the notebook again with a different metric, inducing a different dimensionality**. Some common ones are\n",
        "\n",
        "`[1, 1`]: 2D Euclidean geometry.\n",
        "\n",
        "`[1, 1, 1]` 3D Euclidean geometry.\n",
        "\n",
        "`[1, 1, 1, 1]` 3D Elliptic geometry.\n",
        "\n",
        "`[1, 1, 1, -1]` 3D hyperbolic geometry.\n",
        "\n",
        "`[1, 1, 1, -1]` 4D flat spacetime.\n",
        "\n",
        "Feel free to use these layers in your specialized architecture. In our experiments, we use them as message and update networks in equivariant *graph neural networks*. Check out the main repo for more details!\n",
        "\n",
        "\n",
        "# Links\n",
        "üìú [ArXiV](https://arxiv.org/abs/2305.11141)\n",
        "\n",
        "üñ•Ô∏è [Github](https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks)\n",
        "\n",
        "ü§ì [Blog Posts](https://davidruhe.github.io/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzyv6ElMzVsW"
      },
      "source": [
        "---\n",
        "*Thanks to Chase van de Geijn and Tin Had≈æi Veljkoviƒá for providing feedback to this notebook.*\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}